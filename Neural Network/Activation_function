
Q1. What is an activation function in the context of artificial neural networks?
- An activation function in the context of artificial neural networks is a mathematical function applied to the output of each neuron in a neural network.
- It introduces non-linearity to the network, allowing it to learn complex patterns in the data by transforming the input signal into an output signal.

___________________________________________________________________________________________________________________________________________

Q2. What are some common types of activation functions used in neural networks?
- Sigmoid: Squashes the input values between 0 and 1, used in binary classification tasks.
- Hyperbolic tangent (tanh): Squashes the input values between -1 and 1, similar to the sigmoid function but zero-centered.
- Rectified Linear Unit (ReLU): Returns the input if it is positive, and zero otherwise, commonly used in hidden layers.
- Leaky ReLU: A variant of ReLU that allows a small, non-zero gradient when the input is negative, addressing the vanishing gradient problem.
- Softmax: Used in the output layer for multi-class classification tasks, converts raw scores into probabilities.

___________________________________________________________________________________________________________________________________________

Q3. How do activation functions affect the training process and performance of a neural network?
- Activation functions affect the training process by introducing non-linearity, allowing the network to learn complex relationships in the data.
- They help in mitigating issues such as vanishing gradients and enabling the network to converge during training by providing a range of gradients to update the weights.

___________________________________________________________________________________________________________________________________________

Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?
- Sigmoid squashes the input values between 0 and 1 using the formula \( \frac{1}{1 + e^{-x}} \), where \( x \) is the input to the neuron.
- Advantages: Smooth gradients, suitable for binary classification tasks.
- Disadvantages: Vulnerable to the vanishing gradient problem, not zero-centered, slower optimization in certain cases.

___________________________________________________________________________________________________________________________________________

Q5. What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?
- ReLU returns the input if it is positive, and zero otherwise, defined as \( f(x) = \max(0, x) \).
- Unlike the sigmoid function, ReLU is not bound to a specific range and allows for faster computation due to its simplicity.

___________________________________________________________________________________________________________________________________________

Q6. What are the benefits of using the ReLU activation function over the sigmoid function?
- Faster convergence during training.
- Reduced likelihood of vanishing gradients.
- Simpler computation, leading to faster training times.

___________________________________________________________________________________________________________________________________________

Q7. Explain the concept of "leaky ReLU" and how it addresses the vanishing gradient problem.
- Leaky ReLU is a variant of ReLU that allows a small, non-zero gradient when the input is negative.
- By introducing this small slope, leaky ReLU ensures that neurons do not become inactive during training, addressing the vanishing gradient problem associated with the traditional ReLU function.

___________________________________________________________________________________________________________________________________________

Q8. What is the purpose of the softmax activation function? When is it commonly used?
- Softmax is used in the output layer of a neural network for multi-class classification tasks.
- It converts raw scores or logits into probabilities, ensuring that the output values sum up to 1, representing the probability distribution over multiple classes.

___________________________________________________________________________________________________________________________________________

Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?
- Tanh squashes the input values between -1 and 1 using the formula \( f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \).
- It is similar to the sigmoid function but zero-centered, helping in optimization.
- Tanh generally performs better than the sigmoid function in practice but still suffers from the vanishing gradient problem.
