{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "392108c6-9521-49fe-84e1-e4c921e0da16",
   "metadata": {},
   "source": [
    "### Part 1: Understanding Regularization\n",
    "\n",
    "1. **Regularization in Deep Learning:**\n",
    "   - Regularization refers to a set of techniques used to prevent overfitting in deep learning models. It adds constraints to the optimization problem, encouraging the model to learn simpler patterns and reducing the likelihood of fitting noise in the training data.\n",
    "   - Importance: Overfitting occurs when a model learns to memorize the training data instead of generalizing well to unseen data. Regularization helps mitigate overfitting, leading to better generalization performance on new data.\n",
    "\n",
    "2. **Bias-Variance Tradeoff and Regularization:**\n",
    "   - **Bias:** Error due to overly simplistic assumptions in the model.\n",
    "   - **Variance:** Error due to too much complexity in the model.\n",
    "   - **Tradeoff:** Increasing model complexity (reducing bias) tends to increase variance, leading to overfitting. Regularization introduces constraints that reduce the model's capacity, effectively increasing bias but decreasing variance. This tradeoff helps strike a balance between bias and variance, improving generalization performance.\n",
    "\n",
    "3. **L1 and L2 Regularization:**\n",
    "   - **L1 Regularization (Lasso):** Adds a penalty term proportional to the absolute value of the weights' coefficients. It encourages sparsity in the weight matrix by driving some weights to exactly zero.\n",
    "   - **L2 Regularization (Ridge):** Adds a penalty term proportional to the squared magnitude of the weights' coefficients. It encourages smaller weights but does not usually lead to sparsity.\n",
    "   - **Difference:**\n",
    "     - Penalty Calculation: L1 regularization penalizes the sum of absolute values of weights, while L2 regularization penalizes the sum of squares of weights.\n",
    "     - Effects: L1 regularization tends to produce sparse weight matrices, making it useful for feature selection. L2 regularization encourages smaller weights, reducing the impact of individual weights on the model's output.\n",
    "\n",
    "4. **Role of Regularization in Preventing Overfitting:**\n",
    "   - Regularization constrains the model's capacity, preventing it from fitting the noise in the training data.\n",
    "   - It discourages complex patterns that may not generalize well to new data, promoting simpler models that capture the underlying patterns.\n",
    "   - By penalizing large weights or introducing sparsity, regularization helps prevent overfitting and improves the model's ability to generalize to unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4802ad-756a-463e-97cf-5bf1bfb3919d",
   "metadata": {},
   "source": [
    "### Part 2: Regularization Techniques\n",
    "\n",
    "5. **Dropout Regularization:**\n",
    "   - **Concept:** Dropout is a regularization technique that randomly drops (sets to zero) a fraction of neurons during training. It forces the model to learn redundant representations, reducing reliance on specific neurons and preventing overfitting.\n",
    "   - **Impact on Model Training and Inference:**\n",
    "     - During training, Dropout introduces noise and prevents co-adaptation of neurons, effectively regularizing the model and reducing overfitting.\n",
    "     - During inference, Dropout is turned off, and the full model is used for predictions. However, since neurons were dropped during training, the weights are scaled during inference to maintain the expected outputs, ensuring consistency between training and inference.\n",
    "\n",
    "6. **Early Stopping:**\n",
    "   - **Concept:** Early stopping is a regularization technique that stops training the model when the performance on a validation set starts deteriorating. It prevents overfitting by monitoring the validation loss and stopping training when further optimization leads to worse generalization.\n",
    "   - **Preventing Overfitting:** Early stopping prevents overfitting by stopping the training process before the model starts memorizing noise in the training data. It encourages the model to stop optimizing once it reaches its peak performance on the validation set, ensuring better generalization to unseen data.\n",
    "\n",
    "7. **Batch Normalization:**\n",
    "   - **Concept:** Batch Normalization is a technique used to standardize the inputs of each layer in the network by normalizing the activations. It helps stabilize and speed up the training process by reducing internal covariate shift.\n",
    "   - **Role in Regularization:** Batch Normalization acts as a form of regularization by adding noise to the network during training. It introduces noise through mini-batch statistics, similar to Dropout, which helps prevent overfitting and improves generalization performance.\n",
    "   - **Preventing Overfitting:** Batch Normalization reduces overfitting by regularizing the model through the introduction of noise in the activations. By stabilizing the training process and reducing internal covariate shift, it enables the model to learn more robust and generalizable representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d343de-58b5-4721-b846-91e36b13c71d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce2a9c4b-394c-485f-a717-a7db4ed89f41",
   "metadata": {},
   "source": [
    "# PART 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b74a6b9b-b6ab-4681-9d79-443632ef1329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8041 - loss: 0.6190 - val_accuracy: 0.9553 - val_loss: 0.1484\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9480 - loss: 0.1785 - val_accuracy: 0.9693 - val_loss: 0.1012\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9590 - loss: 0.1320 - val_accuracy: 0.9712 - val_loss: 0.0942\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9641 - loss: 0.1132 - val_accuracy: 0.9727 - val_loss: 0.0866\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9701 - loss: 0.0966 - val_accuracy: 0.9746 - val_loss: 0.0853\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9740 - loss: 0.0852 - val_accuracy: 0.9761 - val_loss: 0.0847\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9758 - loss: 0.0764 - val_accuracy: 0.9782 - val_loss: 0.0803\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9795 - loss: 0.0665 - val_accuracy: 0.9764 - val_loss: 0.0808\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9798 - loss: 0.0670 - val_accuracy: 0.9757 - val_loss: 0.0813\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9795 - loss: 0.0621 - val_accuracy: 0.9784 - val_loss: 0.0823\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9710 - loss: 0.1035\n",
      "Model Performance with Dropout:\n",
      "Test Loss: 0.08535750210285187\n",
      "Test Accuracy: 0.9760000109672546\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to the range [0, 1]\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Reshape the input data to flatten the images\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define model input shape and number of classes\n",
    "input_shape = X_train.shape[1]\n",
    "num_classes = 10  # 10 classes (digits 0-9)\n",
    "\n",
    "# Define the model architecture with Dropout\n",
    "model_with_dropout = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "    layers.Dropout(0.2),  # Dropout layer with 20% dropout rate\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.2),  # Dropout layer with 20% dropout rate\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_with_dropout.compile(optimizer='adam',\n",
    "                           loss='sparse_categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "# Train the model with Dropout\n",
    "history_with_dropout = model_with_dropout.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model with Dropout\n",
    "test_loss_dropout, test_acc_dropout = model_with_dropout.evaluate(X_test, y_test)\n",
    "\n",
    "# Print model performance with Dropout\n",
    "print(\"Model Performance with Dropout:\")\n",
    "print(\"Test Loss:\", test_loss_dropout)\n",
    "print(\"Test Accuracy:\", test_acc_dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60b4e93-6a02-468c-a8c5-fbe93a4ec169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
