{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ffc8a5-39b7-424e-9174-27f70f238ce8",
   "metadata": {},
   "source": [
    "### Part 1: Understanding Weight Initialization\n",
    "\n",
    "1. **Importance of Weight Initialization:**\n",
    "   - Weight initialization is crucial in artificial neural networks because it determines the initial values of the parameters (weights) of the network.\n",
    "   - Careful initialization helps in preventing issues such as vanishing or exploding gradients, which can hinder the training process.\n",
    "   - Proper initialization can lead to faster convergence, better model performance, and improved training stability.\n",
    "\n",
    "2. **Challenges with Improper Weight Initialization:**\n",
    "   - **Vanishing Gradients:** If weights are initialized too small, gradients can become extremely small, leading to slow learning or stagnation during training.\n",
    "   - **Exploding Gradients:** If weights are initialized too large, gradients can become extremely large, causing instability and making it difficult for the model to converge.\n",
    "   - **Poor Convergence:** Improper initialization may lead to slower convergence or convergence to suboptimal solutions, resulting in lower accuracy and performance.\n",
    "\n",
    "3. **Concept of Variance and its Relation to Weight Initialization:**\n",
    "   - Variance represents the spread of values in a distribution. In weight initialization, variance refers to the spread of initial weight values.\n",
    "   - Properly controlling the variance during initialization is crucial because it affects the scale of activations and gradients during training.\n",
    "   - Initializing weights with too high or too low variance can lead to the issues mentioned earlier, impacting the stability and effectiveness of the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af77a45-9875-438a-a1ba-24804d3966b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df118704-4240-46ba-911e-f46318f95cf2",
   "metadata": {},
   "source": [
    "### Part 2: Weight Initialization Techniques\n",
    "\n",
    "4. **Zero Initialization:**\n",
    "   - Zero initialization involves setting all weights in the network to zero initially.\n",
    "   - **Limitations:**\n",
    "     - Potential for Symmetry: If all weights are initialized to the same value, symmetry-breaking properties are lost, leading to symmetric gradients and slow learning.\n",
    "     - Activation Saturation: Zero initialization can cause neurons to output the same value for all inputs, leading to saturation and poor learning.\n",
    "   - **Appropriate Use:**\n",
    "     - Zero initialization can be appropriate in specific cases such as biases or as a starting point for certain layers where symmetry-breaking is not required.\n",
    "\n",
    "5. **Random Initialization:**\n",
    "   - Random initialization involves initializing weights with random values drawn from a specified distribution, such as uniform or normal distribution.\n",
    "   - **Mitigating Issues:**\n",
    "     - Adjusting Variance: By adjusting the variance of the random distribution, it's possible to mitigate issues like vanishing or exploding gradients.\n",
    "     - Using Proper Scaling: Scaling the random initialization based on the number of inputs to a neuron can help maintain stable gradients.\n",
    "     - Choosing Suitable Distributions: Selecting appropriate distributions and parameters can ensure that weights are initialized effectively without saturating or vanishing gradients.\n",
    "\n",
    "6. **Xavier/Glorot Initialization:**\n",
    "   - Xavier/Glorot initialization aims to maintain the variance of activations and gradients throughout the network.\n",
    "   - It initializes weights using a distribution with zero mean and a variance calculated based on the number of input and output units.\n",
    "   - Xavier initialization helps in addressing the challenges of improper weight initialization by ensuring that the gradients neither vanish nor explode during training.\n",
    "\n",
    "7. **He Initialization:**\n",
    "   - He initialization is similar to Xavier initialization but adjusts the variance differently to account for the non-linearity of activation functions like ReLU.\n",
    "   - He initialization sets the variance of weights based on the number of input units only.\n",
    "   - It is preferred over Xavier initialization when using activation functions like ReLU because it helps in preserving the signal and gradients during training more effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91bf4919-7db3-4570-8131-328350bc490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "# Step 2: Load and preprocess the dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0  # Normalize pixel values to range [0, 1]\n",
    "\n",
    "# Step 3: Define the neural network architecture\n",
    "def build_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Step 4: Implement weight initialization techniques\n",
    "def initialize_model(init_type):\n",
    "    model = build_model()\n",
    "    if init_type == 'zero':\n",
    "        # Zero initialization\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, layers.Dense):\n",
    "                layer.kernel_initializer = tf.keras.initializers.Zeros()\n",
    "                \n",
    "    elif init_type == 'random':\n",
    "        # Random initialization (default)\n",
    "        pass  # No need to explicitly set kernel_initializer as it's random by default\n",
    "    \n",
    "    # xaiver/glorot initialization\n",
    "    elif init_type == 'xavier':\n",
    "        # Xavier initialization\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, layers.Dense):\n",
    "                layer.kernel_initializer = tf.keras.initializers.GlorotNormal()\n",
    "    \n",
    "    # He initailization\n",
    "    elif init_type == 'he':\n",
    "        # He initialization\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, layers.Dense):\n",
    "                layer.kernel_initializer = tf.keras.initializers.HeNormal()\n",
    "                \n",
    "    return model\n",
    "\n",
    "# Step 5: Train the models using the chosen dataset\n",
    "def train_model(model, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), verbose=0)\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Step 6: Compare the performance of initialized models\n",
    "def compare_performance():\n",
    "    for init_type in ['zero', 'random', 'xavier', 'he']:\n",
    "        print(f\"\\nTraining model with {init_type} initialization:\")\n",
    "        model = initialize_model(init_type)\n",
    "        history = train_model(model, X_train, y_train, X_test, y_test)\n",
    "        test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print(f\"Test accuracy with {init_type} initialization: {test_acc}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b8fc7b-63d0-4a90-ac84-57f140ea8d95",
   "metadata": {},
   "source": [
    "# Step 7: Discuss considerations and tradeoffs\n",
    "\"\"\"\n",
    "Considerations:\n",
    "- Zero initialization may lead to symmetry issues and poor convergence.\n",
    "- Random initialization is a good default choice but may suffer from saturation or vanishing gradients.\n",
    "- Xavier/Glorot initialization maintains the variance of activations and gradients and is suitable for tanh and sigmoid activations.\n",
    "- He initialization is suitable for ReLU activations and addresses the variance scaling issue of Xavier initialization.\n",
    "\n",
    "Tradeoffs:\n",
    "- Xavier initialization may not work well with ReLU activations as it assumes zero-centered activations.\n",
    "- He initialization may result in exploding gradients if used with deep networks or very large layers.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388ae88a-9472-48cd-abde-3b140563fe0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model with zero initialization:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with zero initialization: 0.9764000177383423\n",
      "\n",
      "Training model with random initialization:\n",
      "Test accuracy with random initialization: 0.9790999889373779\n",
      "\n",
      "Training model with xavier initialization:\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Execute comparison and Step 9: Discuss considerations and tradeoffs\n",
    "compare_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b297970b-79c8-4a2a-b779-8bb5f95d9030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.17,>=2.16\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.28.1)\n",
      "Collecting keras>=3.0.0\n",
      "  Downloading keras-3.0.5-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.11)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ml-dtypes~=0.3.1\n",
      "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Downloading flatbuffers-24.3.7-py2.py3-none-any.whl (26 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Collecting h5py>=3.10.0\n",
      "  Downloading h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Collecting rich\n",
      "  Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dm-tree\n",
      "  Downloading dm_tree-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting namex\n",
      "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2022.12.7)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.9/103.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.1)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.13.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, dm-tree, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, opt-einsum, ml-dtypes, mdurl, markdown, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, markdown-it-py, rich, keras, tensorflow\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.7.0\n",
      "    Uninstalling h5py-3.7.0:\n",
      "      Successfully uninstalled h5py-3.7.0\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 dm-tree-0.1.8 flatbuffers-24.3.7 gast-0.5.4 google-pasta-0.2.0 grpcio-1.62.1 h5py-3.10.0 keras-3.0.5 libclang-16.0.6 markdown-3.5.2 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.3.2 namex-0.0.7 opt-einsum-3.3.0 rich-13.7.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.36.0 termcolor-2.4.0 werkzeug-3.0.1 wrapt-1.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff213378-6365-4d3f-92dc-4443290439d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
