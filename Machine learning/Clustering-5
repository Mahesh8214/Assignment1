Q1. What is a contingency matrix, and how is it used to evaluate the performance of a classification model?
- Contingency Matrix: It's a table showing the distribution of actual classes versus predicted classes. Useful for evaluating the performance of classification models by comparing predicted and actual class assignments.

_____________________________________________________________

Q2. How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in certain situations?
- Pair Confusion Matrix: It's a variation used for binary or two-class problems. It focuses on the pairwise classification outcomes, providing insights into specific error types. Useful for situations where detailed binary classification assessment is needed.

_____________________________________________________________

Q3. What is an extrinsic measure in the context of natural language processing, and how is it typically used to evaluate the performance of language models?
- Extrinsic Measure: It assesses the model's performance based on its contribution to a broader task or application. In NLP, it evaluates how well a language model performs in real-world applications like sentiment analysis or document classification.

_____________________________________________________________

Q4. What is an intrinsic measure in the context of machine learning, and how does it differ from an extrinsic measure?
- Intrinsic Measure: It assesses a model's performance based on its internal characteristics, often without reference to an external task. It's different from an extrinsic measure, which evaluates a model in the context of a specific task or application.

_____________________________________________________________

Q5. What is the purpose of a confusion matrix in machine learning, and how can it be used to identify strengths and weaknesses of a model?
- The confusion matrix summarizes the performance of a classification model by showing true positives, true negatives, false positives, and false negatives. It helps identify strengths (accurate predictions) and weaknesses (misclassifications) of a model.

_____________________________________________________________

Q6. What are some common intrinsic measures used to evaluate the performance of unsupervised learning algorithms, and how can they be interpreted?
- Common intrinsic measures include clustering metrics like silhouette score or Davies-Bouldin index. They help assess the quality of clusters formed by unsupervised learning algorithms. A higher silhouette score indicates better-defined clusters, while a lower Davies-Bouldin index suggests more separable clusters.

_____________________________________________________________

Q7. What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and how can these limitations be addressed?
- Limitations: Accuracy doesn't consider class imbalances and might not reflect a model's performance accurately. For imbalanced datasets, models can achieve high accuracy by simply predicting the majority class. Addressing this involves using additional metrics like precision, recall, F1-score, or area under the ROC curve (AUC-ROC) to provide a more comprehensive evaluation.

_____________________________________________________________
