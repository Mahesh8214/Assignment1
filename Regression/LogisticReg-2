_________________________________________________________________________________________________________________________________________________
Q1. What is the purpose of grid search cv in machine learning, and how does it work?

Purpose: Grid Search Cross-Validation is used for hyperparameter tuning. It systematically searches through a predefined grid
of hyperparameter values, evaluates the model performance using cross-validation, and identifies the set of hyperparameters that
result in the best performance.

Working: It exhaustively tries all possible combinations of hyperparameter values specified in the grid, evaluating the model
for each combination using cross-validation. The combination with the best performance metric is chosen.

_________________________________________________________________________________________________________________________________________________
Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose one over the other?

Grid Search CV: It explores all possible combinations of hyperparameter values from a predefined grid.

Randomized Search CV: It randomly samples hyperparameter values from specified distributions. It's more computationally efficient than grid search.

Choice: If computational resources are limited, or the search space is large, randomized search is often preferred. Grid search is
suitable when you want to exhaustively explore all combinations.

_________________________________________________________________________________________________________________________________________________
Q3. What is data leakage, and why is it a problem in machine learning? Provide an example.

Data Leakage: It occurs when information from outside the training dataset is used to create the model, leading to overly optimistic 
performance estimates.

Example: Suppose you're predicting whether customers will churn, and you accidentally include future information about whether 
they did churn in your training data. This would lead to an overly optimistic assessment of your model's performance.

_________________________________________________________________________________________________________________________________________________
Q4. How can you prevent data leakage when building a machine learning model?

Holdout Data: Use separate datasets for training, validation, and testing.

Time-Based Split: If dealing with time-series data, ensure a temporal order in data splits.

Feature Engineering Caution: Be cautious with feature engineering; ensure no future information is used.

_________________________________________________________________________________________________________________________________________________
Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?

Confusion Matrix: It's a table that describes the performance of a classification model, breaking down predictions
into four categories: true positive, true negative, false positive, and false negative.

Insights: It provides insights into how well a model is performing in terms of precision, recall, accuracy, and other metrics.

_________________________________________________________________________________________________________________________________________________
Q6. Explain the difference between precision and recall in the context of a confusion matrix.

Precision: It measures the accuracy of positive predictions. It's the ratio of true positives to the total predicted positives.

Recall: It measures the ability of the model to capture all the relevant instances. It's the ratio of true positives to the total actual positives.

_________________________________________________________________________________________________________________________________________________
Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?

True Positives (TP): Instances correctly predicted as positive.

True Negatives (TN): Instances correctly predicted as negative.

False Positives (FP): Instances incorrectly predicted as positive.

False Negatives (FN): Instances incorrectly predicted as negative.

Analyzing these components helps identify where the model is making errors.

_________________________________________________________________________________________________________________________________________________
Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?

Accuracy: (TP + TN) / (TP + TN + FP + FN)

Precision: TP / (TP + FP)

Recall (Sensitivity): TP / (TP + FN)

F1 Score: 2 * (Precision * Recall) / (Precision + Recall)

_________________________________________________________________________________________________________________________________________________
Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?

Accuracy: It's the overall correctness of the model and is calculated as (TP + TN) / (TP + TN + FP + FN).

Relationship: Accuracy is influenced by the correct predictions (TP and TN) and is independent of how the model performs on specific classes.

_________________________________________________________________________________________________________________________________________________
Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?

Class Imbalances: If there's a significant class imbalance, the model might be biased towards the majority class.

Misclassifications: Examine which classes are frequently misclassified; this may indicate challenges in distinguishing certain classes.

Analyzing the confusion matrix helps uncover potential issues and guides improvements in the model.
